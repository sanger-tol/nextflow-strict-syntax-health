"""CLI for strict-syntax-health."""

import json
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path

import httpx
import plotly.graph_objects as go
import rich_click as click
from rich.console import Console
from rich.table import Table

# API URLs
PIPELINES_URL = "https://nf-co.re/pipelines.json"
MODULES_REPO_URL = "https://github.com/nf-core/modules.git"

# Directory paths
PIPELINES_DIR = Path("pipelines")
MODULES_DIR = Path("modules")
LINT_RESULTS_DIR = Path("lint_results")

# Pipelines.json now lives inside pipelines/
PIPELINES_JSON_PATH = PIPELINES_DIR / "pipelines.json"

# README path
README_PATH = Path("README.md")

# Lint results subdirectories
PIPELINES_LINT_RESULTS_DIR = LINT_RESULTS_DIR / "pipelines"
MODULES_LINT_RESULTS_DIR = LINT_RESULTS_DIR / "modules"
SUBWORKFLOWS_LINT_RESULTS_DIR = LINT_RESULTS_DIR / "subworkflows"

console = Console()


def _sort_results(results: list[dict]) -> list[dict]:
    """Sort results by parse_error first, then errors (descending), then warnings (descending)."""
    return sorted(results, key=lambda x: (not x.get("parse_error", False), -x["errors"], -x["warnings"]))


def update_pipelines_json() -> None:
    """Download the latest pipelines.json from nf-co.re."""
    console.print(f"Downloading {PIPELINES_URL}...")
    PIPELINES_JSON_PATH.parent.mkdir(parents=True, exist_ok=True)
    response = httpx.get(PIPELINES_URL, timeout=60)
    response.raise_for_status()
    PIPELINES_JSON_PATH.write_bytes(response.content)
    console.print(f"Updated {PIPELINES_JSON_PATH}")


def load_pipelines() -> list[dict]:
    """Load pipelines from the local pipelines.json file."""
    if not PIPELINES_JSON_PATH.exists():
        console.print(f"[red]{PIPELINES_JSON_PATH} not found. Run with --update-pipelines first.[/red]")
        sys.exit(1)

    console.print(f"Loading pipelines from {PIPELINES_JSON_PATH}...")
    data = json.loads(PIPELINES_JSON_PATH.read_text())

    pipelines = []
    for pipeline in data.get("remote_workflows", []):
        if pipeline.get("archived", False):
            continue
        pipelines.append(
            {
                "name": pipeline["name"],
                "full_name": pipeline["full_name"],
                "html_url": f"https://github.com/{pipeline['full_name']}",
            }
        )

    console.print(f"Found {len(pipelines)} active pipelines")
    return pipelines


def clone_modules_repo() -> Path:
    """Clone or update the nf-core/modules repository."""
    if MODULES_DIR.exists():
        console.print("Updating nf-core/modules repository...")
        subprocess.run(
            ["git", "-C", str(MODULES_DIR), "fetch", "--quiet", "origin", "master"],
            check=True,
            capture_output=True,
        )
        subprocess.run(
            ["git", "-C", str(MODULES_DIR), "checkout", "--quiet", "master"],
            check=True,
            capture_output=True,
        )
        subprocess.run(
            ["git", "-C", str(MODULES_DIR), "pull", "--quiet"],
            check=True,
            capture_output=True,
        )
    else:
        console.print("Cloning nf-core/modules repository...")
        subprocess.run(
            ["git", "clone", "--quiet", "--depth", "1", MODULES_REPO_URL, str(MODULES_DIR)],
            check=True,
            capture_output=True,
        )
    console.print(f"nf-core/modules repository ready at {MODULES_DIR}")
    return MODULES_DIR


def discover_modules() -> list[dict]:
    """Discover all modules in the nf-core/modules repository."""
    modules_path = MODULES_DIR / "modules" / "nf-core"
    if not modules_path.exists():
        console.print(f"[red]Modules path not found: {modules_path}[/red]")
        return []

    modules = []
    # Walk through tool directories
    for tool_dir in sorted(modules_path.iterdir()):
        if not tool_dir.is_dir() or tool_dir.name.startswith("."):
            continue
        # Walk through subcommand directories
        for subcommand_dir in sorted(tool_dir.iterdir()):
            if not subcommand_dir.is_dir() or subcommand_dir.name.startswith("."):
                continue
            main_nf = subcommand_dir / "main.nf"
            if main_nf.exists():
                # Module name is tool_subcommand (e.g., bwa_mem)
                name = f"{tool_dir.name}_{subcommand_dir.name}"
                modules.append(
                    {
                        "name": name,
                        "path": subcommand_dir,
                        "html_url": (
                            f"https://github.com/nf-core/modules/tree/master/modules/nf-core/"
                            f"{tool_dir.name}/{subcommand_dir.name}"
                        ),
                    }
                )

    console.print(f"Found {len(modules)} modules")
    return modules


def discover_subworkflows() -> list[dict]:
    """Discover all subworkflows in the nf-core/modules repository."""
    subworkflows_path = MODULES_DIR / "subworkflows" / "nf-core"
    if not subworkflows_path.exists():
        console.print(f"[red]Subworkflows path not found: {subworkflows_path}[/red]")
        return []

    subworkflows = []
    for subworkflow_dir in sorted(subworkflows_path.iterdir()):
        if not subworkflow_dir.is_dir() or subworkflow_dir.name.startswith("."):
            continue
        main_nf = subworkflow_dir / "main.nf"
        if main_nf.exists():
            subworkflows.append(
                {
                    "name": subworkflow_dir.name,
                    "path": subworkflow_dir,
                    "html_url": (
                        f"https://github.com/nf-core/modules/tree/master/subworkflows/nf-core/{subworkflow_dir.name}"
                    ),
                }
            )

    console.print(f"Found {len(subworkflows)} subworkflows")
    return subworkflows


def clone_pipeline(pipeline: dict) -> Path:
    """Clone a pipeline repository, preferring the 'dev' branch."""
    repo_path = PIPELINES_DIR / pipeline["name"]

    if repo_path.exists():
        console.print(f"  Pipeline already cloned: {pipeline['name']}")
        # Try to checkout dev branch, fall back to default if it doesn't exist
        try:
            subprocess.run(
                ["git", "-C", str(repo_path), "fetch", "--quiet", "origin", "dev"],
                check=True,
                capture_output=True,
            )
            subprocess.run(
                ["git", "-C", str(repo_path), "checkout", "--quiet", "dev"],
                check=True,
                capture_output=True,
            )
        except subprocess.CalledProcessError:
            # dev branch doesn't exist, stay on current branch
            pass
        # Pull latest changes
        subprocess.run(
            ["git", "-C", str(repo_path), "pull", "--quiet"],
            check=True,
            capture_output=True,
        )
    else:
        console.print(f"  Cloning {pipeline['full_name']}...")
        PIPELINES_DIR.mkdir(parents=True, exist_ok=True)
        # Try to clone dev branch first
        try:
            subprocess.run(
                ["git", "clone", "--quiet", "--depth", "1", "--branch", "dev", pipeline["html_url"], str(repo_path)],
                check=True,
                capture_output=True,
            )
        except subprocess.CalledProcessError:
            # dev branch doesn't exist, clone default branch
            subprocess.run(
                ["git", "clone", "--quiet", "--depth", "1", pipeline["html_url"], str(repo_path)],
                check=True,
                capture_output=True,
            )

    return repo_path


def get_nextflow_version() -> str:
    """Get the current nextflow version."""
    result = subprocess.run(
        ["nextflow", "-version"],
        capture_output=True,
        text=True,
    )
    # Parse version from output like "nextflow version 24.10.0.5928"
    for line in result.stdout.split("\n"):
        if "version" in line.lower():
            parts = line.split()
            for i, part in enumerate(parts):
                if part.lower() == "version" and i + 1 < len(parts):
                    return parts[i + 1]
    return "unknown"


def lint_component(repo_path: Path, target_path: Path | None = None) -> dict:
    """Run nextflow lint on a component (JSON output for parsing).

    Args:
        repo_path: The repository root path (used as cwd)
        target_path: Optional specific path to lint (relative to repo_path or absolute)
    """
    if target_path:
        # Make path relative to repo_path if it's absolute or inside repo_path
        try:
            relative_path = target_path.relative_to(repo_path)
        except ValueError:
            relative_path = target_path
        cmd = ["nextflow", "lint", str(relative_path), "-o", "json"]
    else:
        cmd = ["nextflow", "lint", ".", "-o", "json"]

    result = subprocess.run(
        cmd,
        cwd=repo_path,
        capture_output=True,
        text=True,
    )

    # nextflow lint returns non-zero exit code if there are errors
    # but we still want to parse the output
    try:
        lint_result = json.loads(result.stdout)
        lint_result["parse_error"] = False
        return lint_result
    except json.JSONDecodeError:
        name = target_path.name if target_path else repo_path.name
        console.print(f"[red]Failed to parse lint output for {name}[/red]")
        console.print(f"stdout: {result.stdout}")
        console.print(f"stderr: {result.stderr}")
        return {"summary": {"errors": 0}, "errors": [], "warnings": [], "parse_error": True}


def lint_pipeline(repo_path: Path) -> dict:
    """Run nextflow lint on a pipeline (JSON output for parsing)."""
    return lint_component(repo_path)


def lint_component_markdown(repo_path: Path, name: str, output_dir: Path, target_path: Path | None = None) -> None:
    """Run nextflow lint on a component and save markdown output to file."""
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / f"{name}_lint.md"

    # Build command - if target_path specified, lint that specific path
    if target_path:
        # Make path relative to repo_path if it's absolute or inside repo_path
        try:
            relative_path = target_path.relative_to(repo_path)
        except ValueError:
            relative_path = target_path
        cmd = ["nextflow", "lint", str(relative_path), "-o", "markdown"]
    else:
        cmd = ["nextflow", "lint", ".", "-o", "markdown"]

    result = subprocess.run(
        cmd,
        cwd=repo_path,
        capture_output=True,
        text=True,
    )

    # Combine stdout and stderr for full output
    output = result.stdout
    if result.stderr:
        output += "\n" + result.stderr

    output_file.write_text(output)
    console.print(f"  Saved lint output to {output_file}")


def run_pipeline_lint(pipelines: list[dict]) -> list[dict]:
    """Clone and lint all pipelines."""
    results = []

    for pipeline in pipelines:
        console.print(f"Processing pipeline {pipeline['name']}...")

        try:
            repo_path = clone_pipeline(pipeline)
            lint_result = lint_pipeline(repo_path)
            lint_component_markdown(repo_path, pipeline["name"], PIPELINES_LINT_RESULTS_DIR)

            results.append(
                {
                    "name": pipeline["name"],
                    "full_name": pipeline["full_name"],
                    "html_url": pipeline["html_url"],
                    "errors": lint_result.get("summary", {}).get("errors", 0),
                    "warnings": len(lint_result.get("warnings", [])),
                    "parse_error": lint_result.get("parse_error", False),
                    "lint_details": lint_result,
                }
            )
        except subprocess.CalledProcessError as e:
            console.print(f"[red]Failed to process {pipeline['name']}: {e}[/red]")
            results.append(
                {
                    "name": pipeline["name"],
                    "full_name": pipeline["full_name"],
                    "html_url": pipeline["html_url"],
                    "errors": 0,
                    "warnings": 0,
                    "parse_error": True,
                    "lint_details": {},
                }
            )

    return results


def run_modules_lint(modules: list[dict]) -> list[dict]:
    """Lint all modules."""
    results = []

    for module in modules:
        console.print(f"Processing module {module['name']}...")

        try:
            lint_result = lint_component(MODULES_DIR, module["path"])
            lint_component_markdown(MODULES_DIR, module["name"], MODULES_LINT_RESULTS_DIR, module["path"])

            results.append(
                {
                    "name": module["name"],
                    "html_url": module["html_url"],
                    "errors": lint_result.get("summary", {}).get("errors", 0),
                    "warnings": len(lint_result.get("warnings", [])),
                    "parse_error": lint_result.get("parse_error", False),
                    "lint_details": lint_result,
                }
            )
        except subprocess.CalledProcessError as e:
            console.print(f"[red]Failed to process module {module['name']}: {e}[/red]")
            results.append(
                {
                    "name": module["name"],
                    "html_url": module["html_url"],
                    "errors": 0,
                    "warnings": 0,
                    "parse_error": True,
                    "lint_details": {},
                }
            )

    return results


def run_subworkflows_lint(subworkflows: list[dict]) -> list[dict]:
    """Lint all subworkflows."""
    results = []

    for subworkflow in subworkflows:
        console.print(f"Processing subworkflow {subworkflow['name']}...")

        try:
            lint_result = lint_component(MODULES_DIR, subworkflow["path"])
            lint_component_markdown(
                MODULES_DIR, subworkflow["name"], SUBWORKFLOWS_LINT_RESULTS_DIR, subworkflow["path"]
            )

            results.append(
                {
                    "name": subworkflow["name"],
                    "html_url": subworkflow["html_url"],
                    "errors": lint_result.get("summary", {}).get("errors", 0),
                    "warnings": len(lint_result.get("warnings", [])),
                    "parse_error": lint_result.get("parse_error", False),
                    "lint_details": lint_result,
                }
            )
        except subprocess.CalledProcessError as e:
            console.print(f"[red]Failed to process subworkflow {subworkflow['name']}: {e}[/red]")
            results.append(
                {
                    "name": subworkflow["name"],
                    "html_url": subworkflow["html_url"],
                    "errors": 0,
                    "warnings": 0,
                    "parse_error": True,
                    "lint_details": {},
                }
            )

    return results


def display_results(results: list[dict], title: str = "nf-core Strict Syntax Health") -> None:
    """Display results in a rich table."""
    table = Table(title=title)
    table.add_column("Pipeline", style="cyan")
    table.add_column("Parse Error", justify="right")
    table.add_column("Errors", justify="right")
    table.add_column("Warnings", justify="right")

    sorted_results = _sort_results(results)

    total_errors = 0
    total_warnings = 0
    total_parse_errors = 0

    for result in sorted_results:
        errors = result["errors"]
        warnings = result["warnings"]
        parse_error = result.get("parse_error", False)

        if parse_error:
            total_parse_errors += 1
            parse_error_str = "[red]Yes[/red]"
            error_str = "-"
            warning_str = "-"
        else:
            total_errors += errors
            total_warnings += warnings
            parse_error_str = "[green]No[/green]"
            error_str = f"[red]{errors}[/red]" if errors > 0 else "[green]0[/green]"
            warning_str = f"[yellow]{warnings}[/yellow]" if warnings > 0 else "[green]0[/green]"

        table.add_row(result["name"], parse_error_str, error_str, warning_str)

    console.print(table)
    console.print(
        f"\n[bold]Total: {total_parse_errors} parse errors, {total_errors} errors, {total_warnings} warnings[/bold]"
    )


def _get_type_dir(type_name: str) -> Path:
    """Get the lint results directory for a specific type."""
    type_dirs = {
        "pipelines": PIPELINES_LINT_RESULTS_DIR,
        "modules": MODULES_LINT_RESULTS_DIR,
        "subworkflows": SUBWORKFLOWS_LINT_RESULTS_DIR,
    }
    return type_dirs[type_name]


def _get_history_path_for_type(type_name: str) -> Path:
    """Get the history file path for a specific type."""
    return _get_type_dir(type_name) / "history.json"


def _get_results_path_for_type(type_name: str) -> Path:
    """Get the results file path for a specific type (for aggregation)."""
    return _get_type_dir(type_name) / "results.json"


def save_results_for_type(type_name: str, results: list[dict]) -> None:
    """Save lint results for a specific type (for later aggregation)."""
    path = _get_results_path_for_type(type_name)
    path.parent.mkdir(parents=True, exist_ok=True)
    # Strip lint_details to keep file size reasonable
    stripped_results = []
    for r in results:
        stripped = {k: v for k, v in r.items() if k != "lint_details"}
        stripped_results.append(stripped)
    path.write_text(json.dumps(stripped_results, indent=2) + "\n")
    console.print(f"Saved results to {path}")


def load_results_for_type(type_name: str) -> list[dict]:
    """Load lint results for a specific type."""
    path = _get_results_path_for_type(type_name)
    if path.exists():
        return json.loads(path.read_text())
    return []


def load_history_for_type(type_name: str) -> list[dict]:
    """Load historical results for a specific type."""
    path = _get_history_path_for_type(type_name)
    if path.exists():
        return json.loads(path.read_text())
    return []


def save_history_for_type(type_name: str, history: list[dict]) -> None:
    """Save historical results for a specific type."""
    path = _get_history_path_for_type(type_name)
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(history, indent=2) + "\n")
    console.print(f"Updated {path}")


def load_history() -> dict:
    """Load historical results from all per-type history files."""
    return {
        "pipelines": load_history_for_type("pipelines"),
        "modules": load_history_for_type("modules"),
        "subworkflows": load_history_for_type("subworkflows"),
    }


def _create_history_entry(results: list[dict]) -> dict:
    """Create a history entry from results."""
    valid_results = [r for r in results if not r.get("parse_error", False)]
    parse_error_results = [r for r in results if r.get("parse_error", False)]

    return {
        "date": datetime.now(timezone.utc).strftime("%Y-%m-%d"),
        "total": len(results),
        "parse_errors": len(parse_error_results),
        "errors_zero": sum(1 for r in valid_results if r["errors"] == 0),
        "errors_low": sum(1 for r in valid_results if 0 < r["errors"] <= 5),
        "errors_high": sum(1 for r in valid_results if r["errors"] > 5),
        "warnings_zero": sum(1 for r in valid_results if r["warnings"] == 0),
        "warnings_low": sum(1 for r in valid_results if 0 < r["warnings"] <= 20),
        "warnings_high": sum(1 for r in valid_results if r["warnings"] > 20),
    }


def _update_history_for_type(history_list: list[dict], entry: dict) -> list[dict]:
    """Update history list for a specific type, replacing today's entry if it exists."""
    today = entry["date"]
    for i, h in enumerate(history_list):
        if h["date"] == today:
            history_list[i] = entry
            return history_list
    history_list.append(entry)
    return history_list


def update_history(
    pipeline_results: list[dict] | None = None,
    module_results: list[dict] | None = None,
    subworkflow_results: list[dict] | None = None,
) -> dict:
    """Add current results to history and return updated history.

    Each type's history is stored in its own file to allow parallel updates.
    """
    history = {}

    if pipeline_results is not None:
        pipelines_history = load_history_for_type("pipelines")
        entry = _create_history_entry(pipeline_results)
        pipelines_history = _update_history_for_type(pipelines_history, entry)
        save_history_for_type("pipelines", pipelines_history)
        history["pipelines"] = pipelines_history

    if module_results is not None:
        modules_history = load_history_for_type("modules")
        entry = _create_history_entry(module_results)
        modules_history = _update_history_for_type(modules_history, entry)
        save_history_for_type("modules", modules_history)
        history["modules"] = modules_history

    if subworkflow_results is not None:
        subworkflows_history = load_history_for_type("subworkflows")
        entry = _create_history_entry(subworkflow_results)
        subworkflows_history = _update_history_for_type(subworkflows_history, entry)
        save_history_for_type("subworkflows", subworkflows_history)
        history["subworkflows"] = subworkflows_history

    return history


def _create_stacked_chart(
    dates: list[str],
    series: list[tuple[list[int], str, str, str]],  # (values, name, line_color, fill_color)
    title: str,
    output_path: Path,
    y_label: str = "Number of Items",
) -> None:
    """Create a stacked area chart and save it to a file."""
    fig = go.Figure()
    for i, (values, name, line_color, fill_color) in enumerate(series):
        fig.add_trace(
            go.Scatter(
                x=dates,
                y=values,
                name=name,
                fill="tozeroy" if i == 0 else "tonexty",
                mode="lines",
                line={"width": 0.5, "color": line_color},
                fillcolor=fill_color,
                stackgroup="stack",
            )
        )
    fig.update_layout(
        title={"text": title, "x": 0.5, "xanchor": "center", "font": {"size": 20}},
        xaxis_title="Date",
        yaxis_title=y_label,
        legend={"orientation": "h", "yanchor": "bottom", "y": 1.02, "xanchor": "center", "x": 0.5},
        template="plotly_white",
        hovermode="x unified",
        width=1000,
        height=500,
    )
    fig.write_image(str(output_path), scale=2)
    console.print(f"Generated {output_path}")


def generate_charts_for_type(history: list[dict], output_dir: Path, type_name: str) -> None:
    """Generate error and warning charts for a specific type (pipelines, modules, subworkflows)."""
    if not history:
        console.print(f"[yellow]Not enough history to generate {type_name} charts[/yellow]")
        return

    output_dir.mkdir(parents=True, exist_ok=True)
    dates = [h["date"] for h in history]
    y_label = f"Number of {type_name.title()}"

    _create_stacked_chart(
        dates,
        [
            ([h["errors_zero"] for h in history], "No errors", "#2ecc71", "rgba(46, 204, 113, 0.7)"),
            ([h["errors_low"] for h in history], "1-5 errors", "#f39c12", "rgba(243, 156, 18, 0.7)"),
            ([h["errors_high"] for h in history], ">5 errors", "#e74c3c", "rgba(231, 76, 60, 0.7)"),
            ([h.get("parse_errors", 0) for h in history], "Parse errors", "#8e44ad", "rgba(142, 68, 173, 0.7)"),
        ],
        f"{type_name.title()} Errors Over Time",
        output_dir / "errors_chart.png",
        y_label,
    )

    _create_stacked_chart(
        dates,
        [
            ([h["warnings_zero"] for h in history], "No warnings", "#1abc9c", "rgba(26, 188, 156, 0.7)"),
            ([h["warnings_low"] for h in history], "1-20 warnings", "#3498db", "rgba(52, 152, 219, 0.7)"),
            ([h["warnings_high"] for h in history], ">20 warnings", "#9b59b6", "rgba(155, 89, 182, 0.7)"),
            ([h.get("parse_errors", 0) for h in history], "Parse errors", "#8e44ad", "rgba(142, 68, 173, 0.7)"),
        ],
        f"{type_name.title()} Warnings Over Time",
        output_dir / "warnings_chart.png",
        y_label,
    )


def generate_all_charts(history: dict) -> None:
    """Generate charts for all types (pipelines, modules, subworkflows)."""
    if history.get("pipelines"):
        generate_charts_for_type(history["pipelines"], PIPELINES_LINT_RESULTS_DIR, "pipelines")
    if history.get("modules"):
        generate_charts_for_type(history["modules"], MODULES_LINT_RESULTS_DIR, "modules")
    if history.get("subworkflows"):
        generate_charts_for_type(history["subworkflows"], SUBWORKFLOWS_LINT_RESULTS_DIR, "subworkflows")


def _generate_results_section(
    results: list[dict],
    type_name: str,
    type_singular: str,
    lint_results_dir: Path,
    include_charts: bool,
) -> list[str]:
    """Generate a results section for a specific type (pipelines, modules, subworkflows)."""
    if not results:
        return []

    sorted_results = _sort_results(results)
    valid_results = [r for r in results if not r.get("parse_error", False)]
    parse_error_count = sum(1 for r in results if r.get("parse_error", False))
    total_errors = sum(r["errors"] for r in valid_results)
    total_warnings = sum(r["warnings"] for r in valid_results)
    zero_error_count = sum(1 for r in valid_results if r["errors"] == 0)
    zero_error_percentage = (zero_error_count / len(results) * 100) if results else 0

    lines = [
        f"## {type_name.title()}",
        "",
        f"- **Total:** {parse_error_count} parse errors, {total_errors} errors, "
        f"{total_warnings} warnings across {len(results)} {type_name}",
        f"- **Zero errors:** {zero_error_count} {type_name} ({zero_error_percentage:.1f}%)",
        "",
    ]

    # Add charts in a side-by-side table
    errors_chart = lint_results_dir / "errors_chart.png"
    warnings_chart = lint_results_dir / "warnings_chart.png"
    if include_charts and errors_chart.exists() and warnings_chart.exists():
        lines.extend(
            [
                "| Errors | Warnings |",
                "|:------:|:--------:|",
                f"| ![Errors]({errors_chart}) | ![Warnings]({warnings_chart}) |",
                "",
            ]
        )

    # Results table in a collapsible details section
    lines.extend(
        [
            "<details>",
            f"<summary>{type_singular.title()} Results ({len(results)} {type_name})</summary>",
            "",
            f"| {type_singular.title()} | Parse Error | Errors | Warnings | Lint Output |",
            "|----------|:-----------:|-------:|---------:|:-----------:|",
        ]
    )

    for result in sorted_results:
        errors = result["errors"]
        warnings = result["warnings"]
        parse_error = result.get("parse_error", False)

        if parse_error:
            parse_error_str = "Yes"
            error_str = "-"
            warning_str = "-"
            status_emoji = ":x:"
        else:
            parse_error_str = "No"
            error_str = str(errors)
            warning_str = str(warnings)
            status_emoji = ":white_check_mark:" if errors == 0 else ":x:"

        name_link = f"{status_emoji} [{result['name']}]({result['html_url']})"
        lint_file_link = f"[View]({lint_results_dir}/{result['name']}_lint.md)"
        lines.append(f"| {name_link} | {parse_error_str} | {error_str} | {warning_str} | {lint_file_link} |")

    lines.extend(
        [
            "",
            "</details>",
            "",
        ]
    )

    return lines


def generate_readme(
    pipeline_results: list[dict] | None = None,
    module_results: list[dict] | None = None,
    subworkflow_results: list[dict] | None = None,
    include_charts: bool = False,
    nextflow_version: str = "unknown",
) -> str:
    """Generate README content with results for all types."""
    now = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")

    lines = [
        "# nf-core Strict Syntax Health Report",
        "",
        "This repository tracks the health of nf-core pipelines, modules, and subworkflows "
        "with respect to Nextflow's _strict syntax_ linting.",
        "",
        "The [Nextflow docs](https://www.nextflow.io/docs/latest/strict-syntax.html) describes the differences "
        "from standard Nextflow syntax and includes many examples to help with migration and fixing errors.",
        "Strict syntax is backwards compatible with existing Nextflow code, "
        "but enforces stricter rules to catch common errors and improve code quality.",
        "",
        "The goal is for all nf-core pipelines to run without errors using strict syntax.",
        "",
        "> [!IMPORTANT]",
        "> See the [nf-core blog post](https://nf-co.re/blog/2025/nextflow_syntax_nf-core_roadmap) "
        "for details on the migration timeline.",
        "> **Fixing all errors from `nextflow lint` will be a requirement by early spring 2026.**",
        "",
        f"- **Last updated:** {now}",
        f"- **Nextflow version:** {nextflow_version}",
        "",
    ]

    # Add sections for each type
    if pipeline_results:
        lines.extend(
            _generate_results_section(
                pipeline_results, "pipelines", "pipeline", PIPELINES_LINT_RESULTS_DIR, include_charts
            )
        )

    if module_results:
        lines.extend(
            _generate_results_section(module_results, "modules", "module", MODULES_LINT_RESULTS_DIR, include_charts)
        )

    if subworkflow_results:
        lines.extend(
            _generate_results_section(
                subworkflow_results, "subworkflows", "subworkflow", SUBWORKFLOWS_LINT_RESULTS_DIR, include_charts
            )
        )

    lines.extend(
        [
            "## About",
            "",
            "This report is generated daily by running `nextflow lint` on each nf-core pipeline, module, "
            "and subworkflow.",
            "The linting checks for strict syntax compliance in Nextflow DSL2 code.",
            "",
            "- **Parse errors** indicate items where `nextflow lint` could not run at all, "
            "typically due to syntax errors that prevent Nextflow from parsing the code",
            "- **Errors** indicate syntax issues that will cause problems in future Nextflow versions",
            "- **Warnings** indicate deprecated patterns that should be updated, "
            "but having warnings is fine (though it's nice to fix those as well if possible)",
            "",
            "## Running Locally",
            "",
            "You can run `nextflow lint` on your own pipeline to check for strict syntax issues:",
            "",
            "```bash",
            "nextflow lint .",
            "```",
            "",
            "> **Note:** Until [this fix](https://github.com/nextflow-io/nextflow/pull/6716) is included "
            "in a Nextflow edge release, you may need to exclude nf-test files manually:",
            "> ```bash",
            '> nextflow lint . -exclude ".git,.nf-test,nf-test.config"',
            "> ```",
            "",
            "See the [strict syntax documentation](https://www.nextflow.io/docs/latest/strict-syntax.html) "
            "for more information about the rules being checked.",
            "",
            "## Getting Help",
            "",
            "If you need help fixing strict syntax errors in your pipeline, "
            "the [Nextflow community forum](https://community.seqera.io/) is a great place to ask questions.",
            "",
        ]
    )

    return "\n".join(lines)


@click.command()
@click.option(
    "--update-readme",
    is_flag=True,
    help="Update the README.md file with the results",
)
@click.option(
    "--update-pipelines",
    is_flag=True,
    help="Download the latest pipelines.json from nf-co.re before running",
)
@click.option(
    "--pipeline",
    "-p",
    multiple=True,
    help="Only process specific pipeline(s) by name (can be used multiple times)",
)
@click.option(
    "--module",
    "-m",
    multiple=True,
    help="Only process specific module(s) by name (can be used multiple times)",
)
@click.option(
    "--subworkflow",
    "-s",
    multiple=True,
    help="Only process specific subworkflow(s) by name (can be used multiple times)",
)
@click.option(
    "--skip-pipelines",
    is_flag=True,
    help="Skip linting pipelines",
)
@click.option(
    "--skip-modules",
    is_flag=True,
    help="Skip linting modules",
)
@click.option(
    "--skip-subworkflows",
    is_flag=True,
    help="Skip linting subworkflows",
)
@click.option(
    "--generate-charts-only",
    is_flag=True,
    help="Only generate charts and README from existing history files (no linting)",
)
def main(
    update_readme: bool,
    update_pipelines: bool,
    pipeline: tuple[str, ...],
    module: tuple[str, ...],
    subworkflow: tuple[str, ...],
    skip_pipelines: bool,
    skip_modules: bool,
    skip_subworkflows: bool,
    generate_charts_only: bool,
) -> None:
    """Check nf-core pipelines, modules, and subworkflows for Nextflow strict syntax linting issues."""
    if update_pipelines:
        update_pipelines_json()

    # Get nextflow version
    nextflow_version = get_nextflow_version()
    console.print(f"Using Nextflow version: {nextflow_version}")

    # Generate charts only mode - skip linting, load existing results and generate outputs
    if generate_charts_only:
        console.print("[bold]Generate charts only mode - loading existing data...[/bold]")

        # Load results from JSON files (saved by lint jobs)
        pipeline_results = load_results_for_type("pipelines") or None
        module_results = load_results_for_type("modules") or None
        subworkflow_results = load_results_for_type("subworkflows") or None

        # Load history and generate charts
        history = load_history()
        generate_all_charts(history)

        if update_readme:
            readme_content = generate_readme(
                pipeline_results=pipeline_results,
                module_results=module_results,
                subworkflow_results=subworkflow_results,
                include_charts=True,
                nextflow_version=nextflow_version,
            )
            README_PATH.write_text(readme_content)
            console.print(f"\n[green]Updated {README_PATH}[/green]")
        return

    pipeline_results: list[dict] | None = None
    module_results: list[dict] | None = None
    subworkflow_results: list[dict] | None = None

    # Lint pipelines
    if not skip_pipelines:
        pipelines = load_pipelines()

        if pipeline:
            pipeline_names = set(pipeline)
            pipelines = [p for p in pipelines if p["name"] in pipeline_names]
            if not pipelines:
                console.print(f"[red]No matching pipelines found for: {', '.join(pipeline_names)}[/red]")
                sys.exit(1)
            console.print(f"Filtering to {len(pipelines)} pipeline(s): {', '.join(p['name'] for p in pipelines)}")

        pipeline_results = run_pipeline_lint(pipelines)
        display_results(pipeline_results, title="nf-core Pipeline Strict Syntax Health")
        # Save results for aggregation (only when not filtering specific pipelines)
        if not pipeline:
            save_results_for_type("pipelines", pipeline_results)

    # Lint modules and subworkflows (requires modules repo)
    if not skip_modules or not skip_subworkflows:
        clone_modules_repo()

        if not skip_modules:
            modules = discover_modules()

            if module:
                module_names = set(module)
                modules = [m for m in modules if m["name"] in module_names]
                if not modules:
                    console.print(f"[red]No matching modules found for: {', '.join(module_names)}[/red]")
                    sys.exit(1)
                console.print(f"Filtering to {len(modules)} module(s): {', '.join(m['name'] for m in modules)}")

            module_results = run_modules_lint(modules)
            display_results(module_results, title="nf-core Module Strict Syntax Health")
            # Save results for aggregation (only when not filtering specific modules)
            if not module:
                save_results_for_type("modules", module_results)

        if not skip_subworkflows:
            subworkflows = discover_subworkflows()

            if subworkflow:
                subworkflow_names = set(subworkflow)
                subworkflows = [s for s in subworkflows if s["name"] in subworkflow_names]
                if not subworkflows:
                    console.print(f"[red]No matching subworkflows found for: {', '.join(subworkflow_names)}[/red]")
                    sys.exit(1)
                console.print(
                    f"Filtering to {len(subworkflows)} subworkflow(s): {', '.join(s['name'] for s in subworkflows)}"
                )

            subworkflow_results = run_subworkflows_lint(subworkflows)
            display_results(subworkflow_results, title="nf-core Subworkflow Strict Syntax Health")
            # Save results for aggregation (only when not filtering specific subworkflows)
            if not subworkflow:
                save_results_for_type("subworkflows", subworkflow_results)

    # Update history and generate charts
    # History is updated per-type when all items of that type are linted (no -p/-m/-s filters)
    # This allows parallel jobs to each update their own history file
    include_charts = False
    history = {}

    # Update history for each type that was fully linted
    if pipeline_results is not None and not pipeline:
        update_history(pipeline_results=pipeline_results)
        history["pipelines"] = load_history_for_type("pipelines")

    if module_results is not None and not module:
        update_history(module_results=module_results)
        history["modules"] = load_history_for_type("modules")

    if subworkflow_results is not None and not subworkflow:
        update_history(subworkflow_results=subworkflow_results)
        history["subworkflows"] = load_history_for_type("subworkflows")

    # Generate charts if any history was updated
    if history:
        # Load full history for chart generation
        full_history = load_history()
        generate_all_charts(full_history)
        include_charts = True

    if update_readme:
        readme_content = generate_readme(
            pipeline_results=pipeline_results,
            module_results=module_results,
            subworkflow_results=subworkflow_results,
            include_charts=include_charts,
            nextflow_version=nextflow_version,
        )
        README_PATH.write_text(readme_content)
        console.print(f"\n[green]Updated {README_PATH}[/green]")


if __name__ == "__main__":
    main()
